# version: '3.8'
volumes:
  shared-workspace:
    name: 'Distributed-FS'
    driver: local


services:
  spark-master:
    hostname: spark-master
    image: spark-cluster
    entrypoint: ['/opt/spark/entrypoint.sh', 'master']
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - shared-workspace:/opt/workspace
    env_file:
      - .env.spark
    ports:
      - "8080:8080"
      - "7077:7077"
      - "2200:22"
    expose:
      - "22"


  spark-history-server:
    hostname: spark-history
    image: spark-cluster
    entrypoint: ['/opt/spark/entrypoint.sh', 'history']
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - shared-workspace:/opt/workspace
      - shared-workspace:/opt/spark/spark-events
    ports:
      - "18080:18080"


  spark-worker:
    image: spark-cluster
    hostname: spark-worker
    entrypoint: ['/opt/spark/entrypoint.sh', 'worker']
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    environment:
      - SPARK_WORKER_CORES=3
      - SPARK_WORKER_MEMORY=1024m
      - SPARK_EXECUTOR_MEMORY=1024m
    volumes:
      - shared-workspace:/opt/workspace


  jupyterlab:
    image: jupyterlab
    hostname: jupyterlab
    container_name: jupyterlab
    ports:
      - 8888:8888
    volumes:
      - shared-workspace:/opt/workspace
